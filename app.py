import streamlit as st
import cv2
import numpy as np
from PIL import Image
import pandas as pd
from datetime import datetime
import plotly.express as px
import random
import os
from geopy.geocoders import Nominatim
from deepface import DeepFace

# ----------------- Page Setup -----------------
st.set_page_config(page_title="Emotion & Location Detection", page_icon="ğŸ‘â€ğŸ—¨", layout="wide", initial_sidebar_state="expanded")

# ----------------- Language Setup -----------------
lang = st.sidebar.selectbox("ğŸŒ Select Language", ["English","ä¸­æ–‡", "Malay"])

translations = {
    "English": {
        "title": "Emotion & Location Recognition System",
        "subtitle": "Try uploading a local photo to analyze emotion and estimate location.",
        "username_prompt": "Enter your username:",
        "logged_in": "Logged in as:",
        "upload_prompt": "Upload an image",
        "detected_emotion": "Detected Emotion",
        "estimated_location": "Estimated Location",
        "start_prompt": "Please enter your username to begin.",
        "nav_home": "Home",
        "nav_location_map": "Location Map",
        "nav_history": "History",
        "nav_filter": "Search & Filter",
        "upload_history": "Upload History",
        "no_history": "No history available yet.",
        "filter_user": "Filter by username (optional):",
        "records_shown": "record(s) shown.",
        "no_record_found": "No record found yet.",
        "enter_username_history": "Please enter your username to view history.",
        "detection_guide": "How Emotion Detection Works",
        "detection_logic": "Detection Logic Explained:",
        "happy_logic": "ğŸ˜Š Happy: Detected when smile is present",
        "angry_logic": "ğŸ˜  Angry: Detected when eyes are wide open and positioned in upper face",
        "neutral_logic": "ğŸ˜ Neutral: Default state when no strong indicators found",
        "sad_logic": "ğŸ˜¢ Sad: Detected when eyes are positioned higher than normal",
        "tips": "Tips for Better Results:",
        "tip1": "Use clear, front-facing images",
        "tip2": "Ensure good lighting",
        "tip3": "Avoid obstructed faces",
        "faces_detected": "face(s) detected",
        "original_image": "Original Image",
        "analysis_result": "Analysis Result",
        "no_faces": "No faces detected"
    },
    "ä¸­æ–‡": {
        "title": "æƒ…ç»ªä¸ä½ç½®è¯†åˆ«ç³»ç»Ÿ",
        "subtitle": "å°è¯•ä¸Šä¼ æœ¬åœ°ç…§ç‰‡ï¼Œä½“éªŒæƒ…ç»ªè¯†åˆ«ä¸ä½ç½®æ¨æµ‹åŠŸèƒ½ã€‚",
        "username_prompt": "è¯·è¾“å…¥ç”¨æˆ·åï¼š",
        "logged_in": "å·²ç™»å½•ç”¨æˆ·ï¼š",
        "upload_prompt": "ä¸Šä¼ å›¾ç‰‡",
        "detected_emotion": "è¯†åˆ«çš„æƒ…ç»ª",
        "estimated_location": "æ¨æµ‹çš„ä½ç½®",
        "start_prompt": "è¯·è¾“å…¥ç”¨æˆ·åä»¥å¼€å§‹ã€‚",
        "nav_home": "ä¸»é¡µ",
        "nav_location_map": "ä½ç½®åœ°å›¾",
        "nav_history": "å†å²è®°å½•",
        "nav_filter": "æŸ¥æ‰¾ & ç­›é€‰",
        "upload_history": "ä¸Šä¼ å†å²",
        "no_history": "æš‚æ— å†å²è®°å½•ã€‚",
        "filter_user": "æŒ‰ç”¨æˆ·åç­›é€‰ï¼ˆå¯é€‰ï¼‰ï¼š",
        "records_shown": "æ¡è®°å½•å·²æ˜¾ç¤ºã€‚",
        "no_record_found": "å°šæœªæ‰¾åˆ°ä»»ä½•è®°å½•ã€‚",
        "enter_username_history": "è¯·è¾“å…¥ç”¨æˆ·åä»¥æŸ¥çœ‹å†å²è®°å½•ã€‚",
        "detection_guide": "æƒ…ç»ªæ£€æµ‹å·¥ä½œåŸç†",
        "detection_logic": "æ£€æµ‹é€»è¾‘è¯´æ˜:",
        "happy_logic": "ğŸ˜Š å¼€å¿ƒ: æ£€æµ‹åˆ°å¾®ç¬‘æ—¶",
        "angry_logic": "ğŸ˜  ç”Ÿæ°”: å½“çœ¼ç›çå¤§ä¸”ä½äºé¢éƒ¨ä¸Šæ–¹æ—¶æ£€æµ‹åˆ°",
        "neutral_logic": "ğŸ˜ ä¸­æ€§: æœªå‘ç°æ˜æ˜¾ç‰¹å¾æ—¶çš„é»˜è®¤çŠ¶æ€",
        "sad_logic": "ğŸ˜¢ æ‚²ä¼¤: å½“çœ¼ç›ä½ç½®æ¯”æ­£å¸¸é«˜æ—¶æ£€æµ‹åˆ°",
        "tips": "è·å–æ›´å¥½ç»“æœçš„æç¤º:",
        "tip1": "ä½¿ç”¨æ¸…æ™°çš„æ­£é¢å›¾åƒ",
        "tip2": "ç¡®ä¿è‰¯å¥½çš„ç…§æ˜",
        "tip3": "é¿å…é¢éƒ¨è¢«é®æŒ¡",
        "faces_detected": "æ£€æµ‹åˆ°äººè„¸",
        "original_image": "åŸå§‹å›¾ç‰‡",
        "analysis_result": "åˆ†æç»“æœ",
        "no_faces": "æœªæ£€æµ‹åˆ°äººè„¸"
    },
    "Malay": {
        "title": "Sistem Pengecaman Emosi dan Lokasi",
        "subtitle": "Cuba muat naik foto tempatan untuk menganalisis emosi dan menganggar lokasi.",
        "username_prompt": "Masukkan nama pengguna anda:",
        "logged_in": "Log masuk sebagai:",
        "upload_prompt": "Muat naik imej",
        "detected_emotion": "Emosi Dikesan",
        "estimated_location": "Lokasi Dianggar",
        "start_prompt": "Sila masukkan nama pengguna untuk bermula.",
        "nav_home": "Halaman Utama",
        "nav_location_map": "Peta Lokasi",
        "nav_history": "Sejarah",
        "nav_filter": "Cari & Tapis",
        "upload_history": "Sejarah Muat Naik",
        "no_history": "Tiada sejarah tersedia buat masa ini.",
        "filter_user": "Tapis mengikut nama pengguna (pilihan):",
        "records_shown": "rekod dipaparkan.",
        "no_record_found": "Tiada rekod dijumpai setakat ini.",
        "enter_username_history": "Sila masukkan nama pengguna untuk melihat sejarah.",
        "detection_guide": "Bagaimana Pengesanan Emosi Berfungsi",
        "detection_logic": "Logik Pengesanan Dijelaskan:",
        "happy_logic": "ğŸ˜Š Gembira: Dikesan apabila senyuman hadir",
        "angry_logic": "ğŸ˜  Marah: Dikesan apabila mata terbuka lebar dan berada di bahagian atas muka",
        "neutral_logic": "ğŸ˜ Neutral: Keadaan lalai apabila tiada penunjuk kuat ditemui",
        "sad_logic": "ğŸ˜¢ Sedih: Dikesan apabila mata berada lebih tinggi daripada biasa",
        "tips": "Petua untuk Hasil yang Lebih Baik:",
        "tip1": "Gunakan imej yang jelas dan menghadap ke hadapan",
        "tip2": "Pastikan pencahayaan yang baik",
        "tip3": "Elakkan muka yang terhalang",
        "faces_detected": "muka dikesan",
        "original_image": "Imej Asal",
        "analysis_result": "Keputusan Analisis",
        "no_faces": "Tiada muka dikesan"
    }
}
T = translations[lang]

# ----------------- Main Title -----------------
st.markdown(f"""
    <h1 style='text-align: center; color: #444444;'>ğŸ‘â€ğŸ—¨ {T['title']}</h1>
    <h4 style='text-align: center; color: #888888;'>{T['subtitle']}</h4>
""", unsafe_allow_html=True)

# ----------------- Tabs -----------------
tabs = st.tabs([
    f"ğŸ  {T['nav_home']}",
    f"ğŸ“Š {T['nav_location_map']}",
    f"ğŸ“‚ {T['nav_history']}",
    f"ğŸ“Š {T['nav_filter']}"
])

# ----------------- Load Models -----------------
@st.cache_resource
def load_models():
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
    return face_cascade, eye_cascade, smile_cascade

face_cascade, eye_cascade, smile_cascade = load_models()

# ----------------- Emotion Detection Functions -----------------
def detect_emotion(img):
    """Detect emotions using OpenCV (happy, neutral, sad, angry)"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    emotions = []
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # Detect smiles
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        # Detect eyes
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # Emotion detection logic
        emotion = "neutral"  # default
        
        # Anger detection
        if len(eyes) >= 2:
            eye_centers = [y + ey + eh/2 for (ex,ey,ew,eh) in eyes[:2]]
            avg_eye_height = np.mean(eye_centers)
            eye_sizes = [eh for (ex,ey,ew,eh) in eyes[:2]]
            avg_eye_size = np.mean(eye_sizes)
            
            if avg_eye_size > h/5 and avg_eye_height < h/2.5:
                emotion = "angry"
            elif avg_eye_height < h/3:
                emotion = "sad"
        
        # Happiness detection (priority)
        if len(smiles) > 0:
            emotion = "happy"
        
        emotions.append(emotion)
    
    return emotions, faces

def draw_detections(img, emotions, faces):
    """Draw detection boxes with labels"""
    output_img = img.copy()
    
    # Color mapping
    color_map = {
        "happy": (0, 255, 0),     # green
        "neutral": (255, 255, 0), # yellow
        "sad": (0, 0, 255),       # red
        "angry": (0, 165, 255)    # orange
    }
    
    for i, ((x,y,w,h), emotion) in enumerate(zip(faces, emotions)):
        color = color_map.get(emotion, (255, 255, 255))
        
        # Draw face rectangle
        cv2.rectangle(output_img, (x,y), (x+w,y+h), color, 3)
        
        # Add emotion label
        cv2.putText(output_img, 
                   emotion.upper(), 
                   (x+5, y-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 
                   0.8, 
                   color, 
                   2)
    
    return output_img

def show_detection_guide():
    """Show detection guide in expandable section"""
    with st.expander(f"â„¹ï¸ {T['detection_guide']}", expanded=False):
        st.markdown(f"""
        **{T['detection_logic']}**
        
        - {T['happy_logic']}
        - {T['angry_logic']}
        - {T['neutral_logic']}
        - {T['sad_logic']}
        
        **{T['tips']}**
        - {T['tip1']}
        - {T['tip2']}
        - {T['tip3']}
        """)

# ----------------- Location Estimation -----------------
def get_location(image):
    """Estimate location based on image metadata or random selection"""
    try:
        # Try to get location from image metadata
        img = Image.open(image)
        info = img._getexif()
        if info and 34853 in info:  # GPSInfo tag
            gps_info = info[34853]
            # Convert GPS coordinates to readable location
            geolocator = Nominatim(user_agent="geo_locator")
            location = geolocator.reverse(f"{gps_info[2][0]}, {gps_info[4][0]}")
            return location.address
    except:
        pass
    
    # Fallback to random location if no metadata
    locations = ["Kuala Lumpur, Malaysia", "Tokyo, Japan", "Paris, France", "New York, USA", "London, UK"]
    return random.choice(locations)

# ----------------- History Management -----------------
def save_history(username, emotion, location):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_record = pd.DataFrame([[username, emotion, location, now]], 
                            columns=["Username", "Emotion", "Location", "timestamp"])
    
    try:
        history_df = pd.read_csv("history.csv")
        history_df = pd.concat([history_df, new_record], ignore_index=True)
    except FileNotFoundError:
        history_df = new_record
    
    history_df.to_csv("history.csv", index=False)

# ----------------- Tab 1: Home -----------------
with tabs[0]:
    username = st.text_input(f"ğŸ‘¤ {T['username_prompt']}")
    if username:
        st.sidebar.success(f"ğŸ‘¤ {T['logged_in']} {username}")
        uploaded_file = st.file_uploader(f"ğŸ“„ {T['upload_prompt']}", type=["jpg", "jpeg", "png"])
        
        if uploaded_file:
            # Convert image format
            image = Image.open(uploaded_file)
            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # Two-column layout
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("ğŸ” Detection Results")
                # Detect emotions
                emotions, faces = detect_emotion(img)
                
                if emotions:
                    # Count each emotion type
                    emotion_count = {}
                    for emo in emotions:
                        emotion_count[emo] = emotion_count.get(emo, 0) + 1
                    
                    # Format the result string
                    result_parts = []
                    for emo, count in emotion_count.items():
                        result_parts.append(f"{count} {emo.capitalize()}")
                    
                    st.success(f"ğŸ­ {T['detected_emotion']}: " + ", ".join(result_parts))
                    
                    # Estimate location
                    location = get_location(uploaded_file)
                    st.info(f"ğŸ“ {T['estimated_location']}: {location}")
                    
                    # Save to history
                    save_history(username, ", ".join(emotions), location)
                    
                    # Show detection guide
                    show_detection_guide()
                else:
                    st.warning(T["no_faces"])
            
            with col2:
                tab1, tab2 = st.tabs([T["original_image"], T["analysis_result"]])
                with tab1:
                    st.image(image, use_container_width=True)
                with tab2:
                    if faces:
                        detected_img = draw_detections(img.copy(), emotions, faces)
                        st.image(detected_img, channels="BGR", use_container_width=True,
                               caption=f"{len(faces)} {T['faces_detected']}")
    else:
        st.warning(T["start_prompt"])

# ----------------- Tab 2: Location Map -----------------
with tabs[1]:
    try:
        history_df = pd.read_csv("history.csv")
        if not history_df.empty:
            # Get unique locations
            unique_locations = history_df['Location'].value_counts().reset_index()
            unique_locations.columns = ['Location', 'Count']
            
            # Generate random coordinates for demo (in real app, use geocoding)
            unique_locations['lat'] = [3.1390 + random.uniform(-0.1, 0.1) for _ in range(len(unique_locations))]
            unique_locations['lon'] = [101.6869 + random.uniform(-0.1, 0.1) for _ in range(len(unique_locations))]
            
            st.map(unique_locations)
        else:
            st.info(T["no_history"])
    except FileNotFoundError:
        st.info(T["no_record_found"])

# ----------------- Tab 3: History -----------------
with tabs[2]:
    st.header(f"ğŸ“œ {T['upload_history']}")
    if username:
        try:
            history_df = pd.read_csv("history.csv")
            if history_df.empty:
                st.info(T["no_history"])
            else:
                keyword = st.text_input(T["filter_user"]).strip()
                if keyword:
                    filtered_df = history_df[history_df["Username"].str.lower() == keyword.lower()]
                else:
                    filtered_df = history_df
                st.dataframe(filtered_df)
                st.caption(f"{len(filtered_df)} {T['records_shown']}")
        except FileNotFoundError:
            st.info(T["no_record_found"])
    else:
        st.warning(T["enter_username_history"])

# ----------------- Tab 4: Filter -----------------
with tabs[3]:
    st.subheader(f"ğŸ§ª {T['nav_filter']}")
    try:
        df = pd.read_csv("history.csv")
        chart = df["Emotion"].value_counts().reset_index()
        chart.columns = ["Emotion", "Count"]
        fig = px.pie(chart, names="Emotion", values="Count", title="Emotion Analysis")
        st.plotly_chart(fig)
    except:
        st.warning("ğŸ“‚ No data available to generate chart.")

    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("**ğŸ”˜ Choose emotion:**")
        emotion = st.radio("Emotion?", ["ğŸ˜Š Happy", "ğŸ˜¢ Sad", "ğŸ˜¡ Angry"], horizontal=True)

        st.markdown("**ğŸ“… Select date:**")
        date = st.date_input("Date of entry")

        st.markdown("**âŒ› Progress bar example:**")
        progress = st.progress(0)
        for i in range(100):
            progress.progress(i + 1)

    with col2:
        st.success("âœ… Everything looks good!")
        st.info("â„¹ï¸ Use left controls to customize analysis")
        st.warning("âš ï¸ No image uploaded yet")

    st.toast("ğŸ”” This is a toast message!", icon="âœ…")
    dummy_data = pd.DataFrame({"Emotion": ["Happy", "Sad"], "Count": [10, 8]})
    st.download_button("â¬‡ï¸ Download Dummy CSV", data=dummy_data.to_csv(), file_name="dummy.csv")
